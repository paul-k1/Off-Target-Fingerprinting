{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Project Off-Target Fingerprinting</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317aa097",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for **Protein Data Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cad24c",
   "metadata": {},
   "source": [
    "This Jupyter Notebook was used to extract data from specific protein structures into a CSV file. Basically, each cell reads an existing CSV, gathers new information, adds it to the existing data, and outputs an updated version of the CSV.\n",
    "\n",
    "Starting from each protein's CHEMBL_ID or UNIPROT_ID, the PDB_IDs of all linked 3D structures were first extracted. Based on those PDB_IDs, relevant data of each PDB_ID was extracted and added to the CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09309336",
   "metadata": {},
   "source": [
    "## Data Retrieval from UniProt and CHEMBL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHEMBL_ID to UNIPROT_ID Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924d4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "\n",
    "input_filename = 'input.csv'\n",
    "output_filename = 'output.csv'\n",
    "\n",
    "# read input\n",
    "with open(input_filename, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=';')\n",
    "    rows = list(reader)\n",
    "    fieldnames = reader.fieldnames if reader.fieldnames else []\n",
    "\n",
    "# add UNIPROT_ID column\n",
    "if 'UNIPROT_ID' not in fieldnames:\n",
    "    fieldnames.append('UNIPROT_ID')\n",
    "\n",
    "# load uniprot_id from chembl websource client api, add the data and save it to a csv file\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    target_client = new_client.target\n",
    "\n",
    "    for row in rows:\n",
    "        chembl_id = row.get('CHEMBL_ID', '').strip()\n",
    "        uniprot_ids = []\n",
    "\n",
    "        try:\n",
    "            results = target_client.filter(target_chembl_id=chembl_id)\n",
    "            if results:\n",
    "                # get all unitprot_ids\n",
    "                components = results[0].get('target_components', [])\n",
    "                for comp in components:\n",
    "                    acc = comp.get('accession')\n",
    "                    if acc:\n",
    "                        uniprot_ids.append(acc)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # leave empty if not found\n",
    "        if not uniprot_ids:\n",
    "            row['UNIPROT_ID'] = ''\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            for uid in uniprot_ids:\n",
    "                new_row = row.copy()\n",
    "                new_row['UNIPROT_ID'] = uid\n",
    "                writer.writerow(new_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696ec9e",
   "metadata": {},
   "source": [
    "## Extract PDB_IDs via UniProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8787dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "input_filename = 'input.csv'\n",
    "output_filename = 'output.csv'\n",
    "\n",
    "# read input\n",
    "with open(input_filename, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=';')\n",
    "    data = list(reader)\n",
    "\n",
    "# add column PDF_ID\n",
    "fieldnames = ['CHEMBL_ID', 'target_nr', 'UNIPROT_ID', 'pref_name', 'gene_symbol', 'target_type', 'PDB_ID']\n",
    "\n",
    "# load PDB_ID from uniprot, add the data and save it to a csv file\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for row in data:\n",
    "        uniprot_id = row.get('UNIPROT_ID', '')\n",
    "\n",
    "        url = \"https://rest.uniprot.org/uniprotkb/\" + uniprot_id\n",
    "        response = requests.get(url)\n",
    "\n",
    "        data = response.json()\n",
    "        pdb_references = [ref for ref in data.get(\"uniProtKBCrossReferences\", []) if ref.get(\"database\") == \"PDB\"]\n",
    "\n",
    "        # export all PDB_IDs\n",
    "        for ref in pdb_references:\n",
    "            pdb_id = ref.get(\"id\")\n",
    "            row[\"PDB_ID\"] = pdb_id\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638f550",
   "metadata": {},
   "source": [
    "## Visualize number of extracted PDB_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847356a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('input.csv', delimiter=';')\n",
    "\n",
    "# count unique PDB_IDs per gene symbol\n",
    "structure_counts = data.groupby('gene_symbol')['PDB_ID'].nunique()\n",
    "\n",
    "plt.figure(figsize=(17, 10))\n",
    "structure_counts.sort_values(ascending=False).plot(kind='bar', width=0.6)\n",
    "plt.title('Number of Structures per Off-Target')\n",
    "plt.xlabel('gene symbol')\n",
    "plt.ylabel('number of structures (PDB_IDs)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72779f",
   "metadata": {},
   "source": [
    "# Use PDBs to extract relevant data\n",
    "\n",
    "We use the PDB_IDs to extract the following data:\n",
    "- Method\n",
    "- Resolution & R-Values\n",
    "- Certain information from the title\n",
    "- Mutation information\n",
    "- Release Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b9440",
   "metadata": {},
   "source": [
    "## Method Extractor\n",
    "\n",
    "X-Ray Diffraction, Electron Microscopy or Solution NMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from aiohttp import ClientSession\n",
    "from asyncio import Semaphore\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "input_df = pd.read_csv('input.csv', delimiter=';')\n",
    "pdb_ids = input_df['PDB_ID'].dropna().unique() # ignore NaN values\n",
    "\n",
    "# use multithreading to retrieve data for different PDBs at the same time\n",
    "# but limit concurrent requests to 5 (prevent overloading the server)\n",
    "MAX_CONCURRENT_REQUESTS = 5\n",
    "semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# async method executed by aiohttp\n",
    "async def fetch_method(pdb_id, session):\n",
    "    url = f'https://data.rcsb.org/rest/v1/core/entry/{pdb_id}'\n",
    "    async with semaphore:\n",
    "        for attempt in range(5):  # retry for max 3 tries\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    if response.status == 200:\n",
    "                        data = await response.json()\n",
    "                        methods = [expt.get('method') for expt in data.get('exptl', [])]\n",
    "                        method = ', '.join(methods)\n",
    "                        return pdb_id, method if method else 'Unknown'\n",
    "                    elif response.status == 429:\n",
    "                        # retry (e.g. when executing too many request at the same time)\n",
    "                        retry_after = int(response.headers.get('Retry-After', 5))\n",
    "                        print(f\"Rate limit exceeded {pdb_id}, wait for {retry_after} seconds.\")\n",
    "                        await asyncio.sleep(retry_after)\n",
    "                    else:\n",
    "                        print(f\"Failed to load data for PDB_ID {pdb_id}: Status {response.status}\")\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to connect to server {pdb_id}, retry {attempt + 1}: {e}\")\n",
    "\n",
    "            # wait and retry\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "            \n",
    "        # In case of any error, write it to the output file\n",
    "        return pdb_id, 'Error'\n",
    "\n",
    "# main method\n",
    "async def get_structure_methods(pdb_ids):\n",
    "    async with ClientSession() as session:\n",
    "        tasks = [fetch_method(pdb_id, session) for pdb_id in pdb_ids]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return {pdb_id: method for pdb_id, method in results}\n",
    "\n",
    "# execute\n",
    "structure_methods = await get_structure_methods(pdb_ids)\n",
    "\n",
    "# convert results to data frame\n",
    "structure_methods_df = pd.DataFrame(list(structure_methods.items()), columns=['PDB_ID', 'Method'])\n",
    "\n",
    "# update and save input_df with new data\n",
    "input_updated_df = input_df.merge(structure_methods_df, on='PDB_ID', how='left')\n",
    "input_updated_df.to_csv('output.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72090532",
   "metadata": {},
   "source": [
    "### Resolution and R-Values Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from aiohttp import ClientTimeout\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# use multithreading to retrieve data for different PDBs at the same time\n",
    "# but limit concurrent requests to 5 (prevent overloading the server)\n",
    "MAX_CONCURRENT_REQUESTS = 10\n",
    "\n",
    "# async method executed by aiohttp\n",
    "async def fetch_pdb_data(session, pdb_id, semaphore, max_retries=3):\n",
    "    url = f\"https://data.rcsb.org/rest/v1/core/entry/{pdb_id}\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with semaphore:\n",
    "                async with session.get(url, timeout=ClientTimeout(total=60)) as response:\n",
    "                    if response.status == 200:\n",
    "                        data = await response.json()\n",
    "                        resolution_list = data.get('rcsb_entry_info', {}).get('resolution_combined', [])\n",
    "                        resolution = resolution_list[0] if resolution_list else None\n",
    "\n",
    "                        # extract r-values\n",
    "                        refine_list = data.get('refine', [])\n",
    "                        if refine_list:\n",
    "                            refine_data = refine_list[0]\n",
    "                            r_free = refine_data.get('ls_rfactor_rfree', None)\n",
    "                            r_work = refine_data.get('ls_rfactor_rwork', None)\n",
    "                            r_observed = refine_data.get('ls_rfactor_obs', None)\n",
    "                        else:\n",
    "                            r_free = r_work = r_observed = None\n",
    "\n",
    "                        return {\n",
    "                            \"resolution\": f\"{float(resolution):.2f}\" if resolution is not None else None,\n",
    "                            \"r_free\": f\"{float(r_free):.3f}\" if r_free is not None else None,\n",
    "                            \"r_work\": f\"{float(r_work):.3f}\" if r_work is not None else None,\n",
    "                            \"r_observed\": f\"{float(r_observed):.3f}\" if r_observed is not None else None,\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch data for {pdb_id}, status code: {response.status}\")\n",
    "        except (asyncio.TimeoutError, aiohttp.ClientError) as e:\n",
    "            print(f\"Attempt {attempt+1}/{max_retries} - Error fetching data for {pdb_id}: {e}\")\n",
    "            await asyncio.sleep(1) \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error fetching data for {pdb_id}: {e}\")\n",
    "            break\n",
    "    print(f\"Failed to fetch data for {pdb_id} after {max_retries} retries\")\n",
    "    return {\n",
    "        \"resolution\": None,\n",
    "        \"r_free\": None,\n",
    "        \"r_work\": None,\n",
    "        \"r_observed\": None,\n",
    "    }\n",
    "\n",
    "async def fetch_all_pdb_data(pdb_ids):\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_pdb_data(session, pdb_id, semaphore) for pdb_id in pdb_ids]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return dict(zip(pdb_ids, results))\n",
    "\n",
    "async def main():\n",
    "    # read input\n",
    "    csv_data = pd.read_csv(\"input.csv\", delimiter=';')\n",
    "\n",
    "    pdb_ids = csv_data['PDB_ID'].dropna().unique()\n",
    "    print(f\"Unique PDB-IDs: {len(pdb_ids)}\")\n",
    "\n",
    "    # split up into batches\n",
    "    batch_size = 1000\n",
    "    all_data = {}\n",
    "\n",
    "    for i in range(0, len(pdb_ids), batch_size):\n",
    "        batch_pdb_ids = pdb_ids[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}: {len(batch_pdb_ids)} PDB IDs\")\n",
    "        batch_data = await fetch_all_pdb_data(batch_pdb_ids)\n",
    "        all_data.update(batch_data)\n",
    "        print(f\"Completed batch {i//batch_size + 1}\")\n",
    "\n",
    "    # extract r values\n",
    "    csv_data['resolution'] = csv_data['PDB_ID'].map(lambda x: all_data.get(x, {}).get('resolution'))\n",
    "    csv_data['r_free'] = csv_data['PDB_ID'].map(lambda x: all_data.get(x, {}).get('r_free'))\n",
    "    csv_data['r_work'] = csv_data['PDB_ID'].map(lambda x: all_data.get(x, {}).get('r_work'))\n",
    "    csv_data['r_observed'] = csv_data['PDB_ID'].map(lambda x: all_data.get(x, {}).get('r_observed'))\n",
    "\n",
    "    # save output file\n",
    "    csv_data.to_csv(\"output.csv\", index=False, sep=';')\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309eacd",
   "metadata": {},
   "source": [
    "### Title Analyzer\n",
    "\n",
    "To obtain clues about potentially complexed ligands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c29322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from aiohttp import ClientSession\n",
    "from asyncio import Semaphore\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "input_df = pd.read_csv('input.csv', delimiter=';')\n",
    "pdb_ids = input_df['PDB_ID'].dropna().unique()\n",
    "\n",
    "MAX_CONCURRENT_REQUESTS = 5\n",
    "semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# The title of each PDB_ID is searched for the following text passages in descending order\n",
    "search_terms = [\n",
    "    \"in complex with an \",\n",
    "    \"in complex with a \",\n",
    "    \"in complex with the \",\n",
    "    \"in complex with \",\n",
    "    \"complexed with \",\n",
    "    \"bound to an \",\n",
    "    \"bound to a \",\n",
    "    \"bound to the \",\n",
    "    \"bound to \",\n",
    "    \"bound with \",\n",
    "    \"with small molecule \",\n",
    "    \"with bound \",\n",
    "    \"with an \",\n",
    "    \"with \"\n",
    "]\n",
    "\n",
    "async def fetch_title_info(pdb_id, session):\n",
    "    url = f'https://data.rcsb.org/rest/v1/core/entry/{pdb_id}'\n",
    "    async with semaphore:\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    if response.status == 200:\n",
    "                        data = await response.json()\n",
    "                        title = data.get('struct', {}).get('title', '').strip()\n",
    "                        \n",
    "                        # check for title_ligand_info\n",
    "                        ligand_info = \"no hint\"\n",
    "                        lower_title = title.lower()\n",
    "                        for term in search_terms:\n",
    "                            if term in lower_title:\n",
    "                                # Text nach dem gefundenen Begriff extrahieren\n",
    "                                after_text = title[lower_title.find(term) + len(term):].strip()\n",
    "                                ligand_info = after_text if after_text else \"no hint\"\n",
    "                                break\n",
    "                        \n",
    "                        # check for title_says_agonist_activator\n",
    "                        inhibitors = [\"agonist\", \"activator\"]\n",
    "                        says_ago_act = \"Yes\" if any(word in lower_title for word in inhibitors) else \"No\"\n",
    "\n",
    "                        # check for title_says_antagonist_inhibitor\n",
    "                        inhibitors = [\"antagonist\", \"inhibitor\"]\n",
    "                        says_anta_inhi = \"Yes\" if any(word in lower_title for word in inhibitors) else \"No\"\n",
    "\n",
    "                        # check for title_says_complex_complexed\n",
    "                        says_complex = \"Yes\" if (\"complex\" in lower_title or \"complexed\" in lower_title) else \"No\"\n",
    "\n",
    "                        return pdb_id, ligand_info, says_ago_act, says_anta_inhi, says_complex\n",
    "                    elif response.status == 429:\n",
    "                        # rate limit, wait and retry\n",
    "                        retry_after = int(response.headers.get('Retry-After', 5))\n",
    "                        await asyncio.sleep(retry_after)\n",
    "                    else:\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "        # default in case everything fails\n",
    "        return pdb_id, \"no hint\", \"No\", \"No\", \"No\"\n",
    "\n",
    "async def process_all(pdb_ids):\n",
    "    async with ClientSession() as session:\n",
    "        tasks = [fetch_title_info(pdb_id, session) for pdb_id in pdb_ids]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "# execute\n",
    "results = asyncio.run(process_all(pdb_ids))\n",
    "\n",
    "# result dataframe\n",
    "columns = ['PDB_ID', 'title_ligand_info', 'title_says_agonist_activator', 'title_says_antagonist_inhibitor', 'title_says_complex_complexed']\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# merge and save results\n",
    "updated_df = input_df.merge(results_df, on='PDB_ID', how='left')\n",
    "updated_df.to_csv('output.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f1c50",
   "metadata": {},
   "source": [
    "### Mutation Information Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0179649",
   "metadata": {},
   "source": [
    "The basic information \"Mutation(s): Yes / No\" is unfortunately not available via the API, so a workaround via browser is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3b8c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from aiohttp import ClientSession\n",
    "from asyncio import Semaphore\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# lock webdriver, only one can be executed at the same time\n",
    "driver_lock = asyncio.Lock()\n",
    "\n",
    "# load data\n",
    "input_df = pd.read_csv('input.csv', delimiter=';')\n",
    "pdb_ids = input_df['PDB_ID'].dropna().unique()\n",
    "\n",
    "# used to execute in parallel\n",
    "MAX_CONCURRENT_REQUESTS = 5\n",
    "semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# start webdriver, openes a chrome browserwindow\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# load mutationinfo from header_mutation\n",
    "async def fetch_method(pdb_id, session):\n",
    "    url = f'https://www.rcsb.org/structure/{pdb_id}'\n",
    "    async with driver_lock:\n",
    "        driver.get(url)\n",
    "        mutation = driver.find_element(By.XPATH, \"//li[contains(@id,'header_mutation')]\").text\n",
    "    return pdb_id, \"Yes\" if \"Yes\" in mutation else \"No\"\n",
    "\n",
    "async def get_mutation_status(pdb_ids):\n",
    "    async with ClientSession() as session:\n",
    "        tasks = [fetch_method(pdb_id, session) for pdb_id in pdb_ids]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return {pdb_id: mutation_status for pdb_id, mutation_status in results}\n",
    "\n",
    "mutation_status = await get_mutation_status(pdb_ids)\n",
    "\n",
    "# merge and save results\n",
    "mutation_status_df = pd.DataFrame(list(mutation_status.items()), columns=['PDB_ID', 'Mutations'])\n",
    "output_df = input_df.merge(mutation_status_df, on='PDB_ID', how='left')\n",
    "\n",
    "output_df.to_csv('output.csv', index=False, sep=';')\n",
    "\n",
    "# close browser window\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8786a0",
   "metadata": {},
   "source": [
    "### Release Date Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d2b1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from aiohttp import ClientSession\n",
    "from asyncio import Semaphore\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "driver_lock = asyncio.Lock()\n",
    "\n",
    "# read input\n",
    "mothersheet_df = pd.read_csv('input.csv', delimiter=';')\n",
    "pdb_ids = mothersheet_df['PDB_ID'].dropna().unique()\n",
    "\n",
    "MAX_CONCURRENT_REQUESTS = 5\n",
    "semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "async def fetch_method(pdb_id, session):\n",
    "    url = f'https://www.rcsb.org/structure/{pdb_id}'\n",
    "    async with driver_lock:\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//li[contains(@id,'header_deposited-released-dates')]\"))\n",
    "            )\n",
    "            dates = element.text\n",
    "            date = dates.split(\"Released:\")[-1].strip()\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            date = \"Not found\"\n",
    "    return pdb_id, date\n",
    "\n",
    "async def get_release_dates(pdb_ids):\n",
    "    async with ClientSession() as session:\n",
    "        tasks = [fetch_method(pdb_id, session) for pdb_id in pdb_ids]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return {pdb_id: release_date for pdb_id, release_date in results}\n",
    "\n",
    "release_dates = await get_release_dates(pdb_ids)\n",
    "\n",
    "# merge and save results\n",
    "release_dates_df = pd.DataFrame(list(release_dates.items()), columns=['PDB_ID', 'Release Date'])\n",
    "output_df = mothersheet_df.merge(release_dates_df, on='PDB_ID', how='left')\n",
    "\n",
    "output_df.to_csv('output.csv', index=False, sep=';')\n",
    "\n",
    "# close browser window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33639d9",
   "metadata": {},
   "source": [
    "After successfull execution of all data extraction cells, the resulting CSV file was converted to an .xlsx and the data was seperated into different sheets within this file. Therefore, the Excel macro SplitByTarget was used, to generate one sheet per target and seperate the data into the corresponding sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e7c93",
   "metadata": {},
   "source": [
    "## Download of selected 3D Structures (.cif files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2cc99",
   "metadata": {},
   "source": [
    "### Generate download links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75005dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_path = \"input.csv\"\n",
    "output_path = \"links.csv\"\n",
    "base_url = \"https://files.rcsb.org/download/{}.cif\"\n",
    "\n",
    "with open(input_path, newline=\"\", encoding=\"utf-8-sig\") as infile, open(output_path, \"w\", newline=\"\") as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    for row in reader:\n",
    "        if not row or not row[0].strip():\n",
    "            continue\n",
    "        pdb_id = row[0].strip().upper()\n",
    "        writer.writerow([base_url.format(pdb_id)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee803819",
   "metadata": {},
   "source": [
    "### Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1267415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "csv_path = \"links.csv\"\n",
    "output_dir = \"cifs\"\n",
    "max_retries = 5\n",
    "timeout = 10\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "df = pd.read_csv(csv_path, header=None)\n",
    "\n",
    "for index, url in enumerate(df[0], start=1):\n",
    "    filename = url.strip().split(\"/\")[-1]\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    print(f\"[{index}/{len(df)}] Loading {filename} ...\", end=\" \")\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            with open(output_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Done\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                print(f\"Error ({e})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
